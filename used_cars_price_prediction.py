# -*- coding: utf-8 -*-
"""Used_cars_price_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/VM-137/used-car-price-prediction/blob/main/Used_cars_price_prediction.ipynb

# Understanding the problem statement and business case
* In this hand-on project, we will train 3 Machine Learning algorithms namely Multiple Linear Regression, Random Forest Regression and XGBoost to predict the price of used cars.
* INPUTS(FEATURES): Make, Model, Type, Origin, Drivetrain, Invoice, EngineSize, Cylinders, Horesepower, MPG_City, MPG_Highway, Weight, Wheelbase and Lenght.
* OUTPUT: MSRP (price)
* This project can be used by car dealerships to predict used car prices and understand key factors that contribute to used car prices.

# Libraries/dataset import
"""

!pip install xgboost
!pip install wordcloud
!pip install plotly

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from wordcloud import WordCloud, STOPWORDS
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score
from math import sqrt

"""Import dataset from github"""

!wget https://github.com/Krishcodes56/used-car-price-prediction-google-colab-project.git

car_df = pd.read_csv('/content/CARS.csv')

car_df.head(10)

"""Display features columns

# Data Preprocessing (1)
* Check features
* Check shape
* Check missing values
* Check data types

### Features
"""

car_df.columns

"""Check the shape and missing values

### Shape
"""

car_df.shape

"""### Missing values"""

car_df.isna().sum()

"""There are just 2 missing values, we will drop that two rows"""

car_df = car_df.dropna()

car_df.info()

"""### Data types
* Convert 'MSRP' and 'Invoice' datatype to integer, we need to remove '$' str and comma ',' from these 2 columns
"""

car_df['MSRP'] = car_df['MSRP'].str.replace('$', '')
car_df['MSRP'] = car_df['MSRP'].str.replace(',', '')
car_df['MSRP'] = car_df['MSRP'].astype(int)
car_df['Invoice'] = car_df['Invoice'].str.replace('$', '')
car_df['Invoice'] = car_df['Invoice'].str.replace(',', '')
car_df['Invoice'] = car_df['Invoice'].astype(int)

car_df[['MSRP', 'Invoice']].head(10)

car_df.info()

"""# Exploratory Data Analysis (1)
* Dataset summary
* Scatter plots and histograms
* 'Model' words cloud visualization

### Dataset summary
* Coun, mean, std, min, max of each feature
"""

car_df.describe()

print('Maximum price for used car: ', car_df['MSRP'].max())

print('Minimum price for used car: ', car_df['MSRP'].min())

"""### Visualization

* Scatter plots for relationship and histograms for univariate distributions
"""

sns.pairplot(data=car_df)

"""What's going on?
It seems that there is a linear trend between 'Horsepower' and 'MSRP' (the price), probably it have a direct impact on the price.
We can analyze other features and see that there seems to be an inverse relation between 'MPG_' and 'Horsepower' or 'MPG_'and 'Enginesize' which makes sense as far as we are talking about normal cars.

* Let's view unique makes of the cars
"""

car_df.Make.unique()

fig = px.histogram(car_df, x = 'Make',
                   labels = {'Make': 'Manufacturer'},
                   title = 'Make OF THE CAR',
                   color_discrete_sequence = ['maroon']
                   )
fig.show()

"""* Let's view various types of cars"""

car_df.Type.unique()

fig = px.histogram(car_df, x='Type',
                   labels = {'Type':'Type'},
                   title = 'TYPE OF CAR',
                   color_discrete_sequence = ['brown']
                   )
fig.show()

"""* Let's view the location of the car sales"""

car_df.Origin.unique()

fig = px.histogram(car_df, x='Origin',
                   labels = {'Origin':'Origin'},
                   title = 'LOCATION OF THE CAR SALES',
                   color_discrete_sequence= ['brown']
                   )
fig.show()

"""* Let's view the drivetrain of the cars"""

car_df.DriveTrain.unique()

fig = px.histogram(car_df, x = 'DriveTrain',
                   labels = {'DriveTrain':'DriveTrain'},
                   title = 'DRIVETRAIN OF THE CAR',
                   color_discrete_sequence = ['BLACK']
                   )
fig.show()

"""* Plot of the make of the car and its location"""

fig = px.histogram(car_df, x = 'Make',
                   color = 'Origin',
                   labels = {'Make':'Manufacturer'},
                   title = 'MAKE OF THE CAR Vs LOCATION'
                   )
fig.show()

"""* Plot of the make of the car and its type"""

fig = px.histogram(car_df, x = 'Make',
                   color = 'Type',
                   labels = {'Make':'Manufacturer'},
                   title = 'MAKE OF THE CAR Vs TYPE'
                   )
fig.show()

"""* Let's analyze the distribution of words in the 'Model' column using WordCloud generator"""

text = car_df['Model'].values
stopwords = set(STOPWORDS)

wc = WordCloud(background_color='black',
               max_words=2000,
               max_font_size=100,
               random_state=3,
               stopwords=stopwords,
               contour_width=3
               ).generate(str(text))

fig = plt.figure(figsize = (25, 15))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

"""We can visualize the most common words used in the model name.

* Correlation matrix
"""

car_df.corr()

sns.heatmap(car_df.corr(), annot=True)

"""We can see that the feature with highest correlation to 'MSRP' seems to be 'Horsepower' with 0.83 as we have suspected, the inverse happens with 'MPG_' and 'Enginesize' with negative (~0.7) correlation.

# Data Preprocessing (2)
* Perform One-Hot Encoding for 'Make', 'Model', 'Type', 'Origin' and 'DriveTrain'
* Drop 'Invoice' column as it does not contribute.
* Split the dataset into Train and Test parts

* One-hot encoding
"""

df_dum = pd.get_dummies(car_df,
                        columns = ['Make',
                                   'Model',
                                   'Type',
                                   'Origin',
                                   'DriveTrain']
                        )

df_dum.head(10)

"""* Drop 'Invoice' column"""

df_data = df_dum.drop(columns=['Invoice'], axis=1)

df_data.head()

print('DataFrame shape = ',df_data.shape)

"""Split dependent and independent variables"""

X = df_data.drop(columns=['MSRP'], axis=1)
y = df_data['MSRP']

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
print('X Test dataset shape = ', X_train.shape)
print('X Train dataset shape = ', X_test.shape)
print('y Test dataset shape = ', y_train.shape)
print('y Train dataset shape = ', y_test.shape)

"""# Model (1)
* Linear Regression
"""

L_model = LinearRegression()
L_model.fit(X_train, y_train)

accuracy_L_model = L_model.score(X_test, y_test)
print('Score = ', accuracy_L_model)



"""# Model (2)
* Decision tree
* Random forest

* Decision Tree
"""

DecisionTree_model = DecisionTreeRegressor()
DecisionTree_model.fit(X_train, y_train)

accuracy_DecisionTree = DecisionTree_model.score(X_test, y_test)
print('Score Decision Tree = ', accuracy_DecisionTree)

"""* Random forest"""

RandomForest_model = RandomForestRegressor(n_estimators=5, max_depth=5)
RandomForest_model.fit(X_train, y_train)

accuracy_RandomForest = RandomForest_model.score(X_test, y_test)
print('Score Random Forest = ', accuracy_RandomForest)

"""# Model (3)
* XGBoost
"""

model = XGBRegressor(objective ='reg:squarederror',
                     base_score=0.5,
                     learning_rate=0.15
                     )
model.fit(X_train, y_train)

accuracy_XGBoost = model.score(X_test, y_test)
print('Score XGBoost = ', accuracy_XGBoost)

"""# Model comparison

* Linear regression
"""

y_predict_linear = L_model.predict(X_test)
fig = sns.regplot(x=y_predict_linear, y=y_test, color='red', marker='^')
fig.set(title='Linear Regression Model',
        xlabel='Predicted Price of the used cars ($)',
        ylabel = 'Actual price of the used cars ($)'
        )

RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict_linear))))
MSE = mean_squared_error(y_test, y_predict_linear)
MAE = mean_absolute_error(y_test, y_predict_linear)
r2 = r2_score(y_test, y_predict_linear)
print('RMSW =', RMSE,'\nMSE =', MSE, '\nMAE =', MAE, '\nR2 =', r2)

"""* Random forest"""

y_predict_RandomForest = RandomForest_model.predict(X_test)
fig = sns.regplot(x=y_predict_RandomForest, y=y_test, color='blue', marker='s')
fig.set(title='Random Forest Regression Model',
        xlabel='Predicted Price of the used cars($)',
        ylabel = 'Actual price of the used cars ($)'
        )

RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict_RandomForest))))
MSE = mean_squared_error(y_test, y_predict_RandomForest)
MAE = mean_absolute_error(y_test, y_predict_RandomForest)
r2 = r2_score(y_test, y_predict_RandomForest)
print('RMSW =', RMSE,'\nMSE =', MSE, '\nMAE =', MAE, '\nR2 =', r2)

"""* XGBoost"""

y_predict_XGBoost = model.predict(X_test)
fig = sns.regplot(x=y_predict_XGBoost, y=y_test, color='blue', marker='s')
fig.set(title='XGBoost Model',
        xlabel='Predicted Price of the used cars($)',
        ylabel = 'Actual price of the used cars ($)'
        )

RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict_XGBoost))))
MSE = mean_squared_error(y_test, y_predict_XGBoost)
MAE = mean_absolute_error(y_test, y_predict_XGBoost)
r2 = r2_score(y_test, y_predict_XGBoost)
print('RMSW =', RMSE,'\nMSE =', MSE, '\nMAE =', MAE, '\nR2 =', r2)